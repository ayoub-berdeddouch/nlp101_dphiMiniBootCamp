{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP 101 - 01 - Text Wrangling visit [ LINK ](https://www.kaggle.com/ayoubberdeddouch/nlp101-01-textwrangling)","metadata":{}},{"cell_type":"markdown","source":"# NLP 101 - 02 - Text Representation with Feature Engineering\n\n### Exploring Traditional Statistical Models\n\nFeature Engineering is often known as the secret sauce to creating superior and better performing machine learning models. Just one excellent feature could be your ticket to winning a Kaggle challenge! The importance of feature engineering is even more important for unstructured, textual data because we need to convert free flowing text into some numeric representations which can then be understood by machine learning algorithms. \n\nHere we will explore the following feature engineering techniques:\n\n- Bag of Words Model (TF)\n- Bag of N-grams Model\n- TF-IDF Model\n- Similarity Features","metadata":{"id":"-dWlnZ1vRP6H"}},{"cell_type":"markdown","source":"# Prepare a Sample Corpus\n\nLet’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains.","metadata":{"id":"NYC_RfbeRP6J"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\npd.options.display.max_colwidth = 200\n\ncorpus = ['The sky is blue and beautiful.',\n          'Love this blue and beautiful sky!',\n          'The quick brown fox jumps over the lazy dog.',\n          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n          'I love green eggs, ham, sausages and bacon!',\n          'The brown fox is quick and the blue dog is lazy!',\n          'The sky is very blue and the sky is very beautiful today',\n          'The dog is lazy but the brown fox is quick!'    \n]\nlabels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n\ncorpus = np.array(corpus)\ncorpus_df = pd.DataFrame({'Document': corpus, \n                          'Category': labels})\ncorpus_df = corpus_df[['Document', 'Category']]\ncorpus_df","metadata":{"id":"6xn8eDqARP6J","outputId":"8ee06c2b-ddf7-4e5a-a23c-9b8038d3d9b2","execution":{"iopub.status.busy":"2021-09-25T16:35:37.396515Z","iopub.execute_input":"2021-09-25T16:35:37.396982Z","iopub.status.idle":"2021-09-25T16:35:37.452629Z","shell.execute_reply.started":"2021-09-25T16:35:37.396875Z","shell.execute_reply":"2021-09-25T16:35:37.451835Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"You can see that we have taken a few sample text documents belonging to different categories for our toy corpus. Before we talk about feature engineering, as always, we need to do some data pre-processing or wrangling to remove unnecessary characters, symbols and tokens.","metadata":{"id":"QOMGTkvCRP6N"}},{"cell_type":"markdown","source":"# Simple Text Pre-processing\n\nSince the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus.","metadata":{"id":"nAcT9NHkRP6O"}},{"cell_type":"markdown","source":"## [Check this notebool for Text Wrangling](https://www.kaggle.com/ayoubberdeddouch/nlp101-01-textwrangling)","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"id":"vSIxl_PUSCvj","outputId":"6477f365-d714-461e-bac4-cf4bc17705f5","execution":{"iopub.status.busy":"2021-09-25T16:39:16.431935Z","iopub.execute_input":"2021-09-25T16:39:16.432341Z","iopub.status.idle":"2021-09-25T16:39:18.063034Z","shell.execute_reply.started":"2021-09-25T16:39:16.432304Z","shell.execute_reply":"2021-09-25T16:39:18.062016Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re\n\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = nltk.word_tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)\n\nnorm_corpus = normalize_corpus(corpus)\nnorm_corpus","metadata":{"id":"OElcyG9WRP6P","outputId":"fcfae9fa-349d-4e77-9f75-e5f241dfe5d9","execution":{"iopub.status.busy":"2021-09-25T16:39:55.189805Z","iopub.execute_input":"2021-09-25T16:39:55.190363Z","iopub.status.idle":"2021-09-25T16:39:55.219683Z","shell.execute_reply.started":"2021-09-25T16:39:55.190319Z","shell.execute_reply":"2021-09-25T16:39:55.218865Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words Model - TF\n\nThis is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\\attribute. The bag of words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0) or even weighted values. The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar.\n![BOW](https://dataaspirant.com/wp-content/uploads/2021/01/1-Bag-of-words.png)","metadata":{"id":"MMBWGlKTRP6T"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(min_df=0., max_df=1.)\ncv_matrix = cv.fit_transform(norm_corpus)\ncv_matrix = cv_matrix.toarray()\ncv_matrix","metadata":{"id":"d7mAqxTlRP6U","outputId":"f771b092-0d12-48c5-9182-5beb92643376","execution":{"iopub.status.busy":"2021-09-25T16:40:04.266154Z","iopub.execute_input":"2021-09-25T16:40:04.266496Z","iopub.status.idle":"2021-09-25T16:40:04.279207Z","shell.execute_reply.started":"2021-09-25T16:40:04.266464Z","shell.execute_reply":"2021-09-25T16:40:04.278409Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Thus you can see that our documents have been converted into numeric vectors such that each document is represented by one vector (row) in the above feature matrix. The following code will help represent this in a more easy to understand format.","metadata":{"id":"lPp6Bzs7RP6W"}},{"cell_type":"code","source":"# get all unique words in the corpus\nvocab = cv.get_feature_names()\n# show document feature vectors\npd.DataFrame(cv_matrix, columns=vocab)","metadata":{"id":"FVsdSiwARP6W","outputId":"03bf4798-1714-41d8-fc3c-6a93d003c393","execution":{"iopub.status.busy":"2021-09-25T16:40:10.548005Z","iopub.execute_input":"2021-09-25T16:40:10.548378Z","iopub.status.idle":"2021-09-25T16:40:10.568173Z","shell.execute_reply.started":"2021-09-25T16:40:10.548339Z","shell.execute_reply":"2021-09-25T16:40:10.567297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This should make things more clearer! You can clearly see that each column or dimension in the feature vectors represents a word from the corpus and each row represents one of our documents. The value in any cell, represents the number of times that word (represented by column) occurs in the specific document (represented by row). Hence if a corpus of documents consists of N unique words across all the documents, we would have an N-dimensional vector for each of the documents.","metadata":{"id":"A_1k60ZERP6Z"}},{"cell_type":"markdown","source":"# Bag of N-Grams Model\n\nA word is just a single token, often known as a unigram or 1-gram. We already know that the Bag of Words model doesn’t consider order of words. But what if we also wanted to take into account phrases or collection of words which occur in a sequence? N-grams help us achieve that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is hence just an extension of the Bag of Words model so we can also leverage N-gram based features. The following example depicts bi-gram based features in each document feature vector.","metadata":{"id":"0DfPJwSsRP6Z"}},{"cell_type":"code","source":"# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\nbv = CountVectorizer(ngram_range=(2,2))\nbv_matrix = bv.fit_transform(norm_corpus)\n\nbv_matrix = bv_matrix.toarray()\nvocab = bv.get_feature_names()\npd.DataFrame(bv_matrix, columns=vocab)","metadata":{"id":"4n1N32JyRP6a","outputId":"5c091718-a76c-448c-add9-311f32486230","execution":{"iopub.status.busy":"2021-09-25T16:40:15.661906Z","iopub.execute_input":"2021-09-25T16:40:15.662215Z","iopub.status.idle":"2021-09-25T16:40:15.844078Z","shell.execute_reply.started":"2021-09-25T16:40:15.662185Z","shell.execute_reply":"2021-09-25T16:40:15.842754Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"This gives us feature vectors for our documents, where each feature consists of a bi-gram representing a sequence of two words and values represent how many times the bi-gram was present for our documents.","metadata":{"id":"bW6F4h8QRP6c"}},{"cell_type":"markdown","source":"# TF-IDF Model\n\nThere are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these may tend to overshadow other terms in the feature set. The TF-IDF model tries to combat this issue by using a scaling or normalizing factor in its computation. ___TF-IDF___ stands for __Term Frequency-Inverse Document Frequency__, which uses a combination of two metrics in its computation, namely: ___term frequency (tf)___ and ___inverse document frequency (idf)___. This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of information retrieval and NLP.\n\nMathematically, we can define TF-IDF as ___tfidf = tf x idf___, which can be expanded further to be represented as follows.\n\n![](https://ichi.pro/assets/images/max/724/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)\n\nHere, ___tfidf(w, D)___ is the TF-IDF score for word __w__ in document __D__. \n- The term ___tf(w, D)___ represents the term frequency of the word __w__ in document __D__, which can be obtained from the Bag of Words model. \n- The term ___idf(w, D)___ is the inverse document frequency for the term __w__, which can be computed as the log transform of the total number of documents in the corpus __C__ divided by the document frequency of the word __w__, which is basically the frequency of documents in the corpus where the word __w__ occurs. \n\nThere are multiple variants of this model but they all end up giving quite similar results. Let’s apply this on our corpus now!","metadata":{"id":"pPwKOnnkRP6d"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\ntv_matrix = tv.fit_transform(norm_corpus)\ntv_matrix = tv_matrix.toarray()\n\nvocab = tv.get_feature_names()\npd.DataFrame(np.round(tv_matrix, 2), columns=vocab)","metadata":{"id":"sFp9zDbgRP6d","outputId":"009f142f-1b99-4bfd-9e78-2bbf9db03807","execution":{"iopub.status.busy":"2021-09-25T16:52:52.846299Z","iopub.execute_input":"2021-09-25T16:52:52.847272Z","iopub.status.idle":"2021-09-25T16:52:52.897610Z","shell.execute_reply.started":"2021-09-25T16:52:52.847216Z","shell.execute_reply":"2021-09-25T16:52:52.896608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**The TF-IDF based feature vectors for each of our text documents show scaled and normalized values as compared to the raw Bag of Words model values.**","metadata":{"id":"0jvKPWIwRP6g"}},{"cell_type":"markdown","source":"# Document Similarity\n\nDocument similarity is the process of using a distance or similarity based metric that can be used to identify how similar a text document is with any other document(s) based on features extracted from the documents like bag of words or tf-idf.\n\nThus you can see that we can build on top of the tf-idf based features we engineered in the previous section and use them to generate new features which can be useful in domains like search engines, document clustering and information retrieval by leveraging these similarity based features.\n\nPairwise document similarity in a corpus involves computing document similarity for each pair of documents in a corpus. Thus if you have C documents in a corpus, you would end up with a C x C matrix such that each row and column represents the similarity score for a pair of documents, which represent the indices at the row and column, respectively. There are several similarity and distance metrics that are used to compute document similarity. These include cosine distance/similarity, euclidean distance, manhattan distance, BM25 similarity, jaccard distance and so on. In our analysis, we will be using perhaps the most popular and widely used similarity metric,\ncosine similarity and compare pairwise document similarity based on their TF-IDF feature vectors.","metadata":{"id":"7xzqFWAkRP6h"}},{"cell_type":"markdown","source":"----\n**Cosine similarity** basically gives us a metric representing the cosine of the angle between the feature vector representations of two text documents. Lower the angle between the documents, the closer and more similar they are as depicted in the following figure.\n\n![](https://www.machinelearningplus.com/wp-content/uploads/2018/10/3d_projection.png)\n\n\nLooking closely at the similarity matrix clearly tells us that documents (0, 1 and 6), (2, 5 and 7) are very similar to one another and documents 3 and 4 are slightly similar to each other but the magnitude is not very strong, however still stronger than the other documents. This must indicate these similar documents have some similar features. This is a perfect example of grouping or clustering that can be solved by unsupervised learning especially when you are dealing with huge corpora of millions of text documents.","metadata":{"id":"Uq737P7GRP6l"}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_matrix = cosine_similarity(tv_matrix)\nsimilarity_df = pd.DataFrame(similarity_matrix)\nsimilarity_df","metadata":{"id":"zyToCxp9RP6i","outputId":"f9fa6f92-dded-4155-86fe-e6ebc8a5e8a7","execution":{"iopub.status.busy":"2021-09-25T16:53:06.431949Z","iopub.execute_input":"2021-09-25T16:53:06.432667Z","iopub.status.idle":"2021-09-25T16:53:06.453886Z","shell.execute_reply.started":"2021-09-25T16:53:06.432617Z","shell.execute_reply":"2021-09-25T16:53:06.452841Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Clustering using Document Similarity Features\n\nWe will use a very popular partition based clustering method, K-means clustering to cluster or group these documents based on their similarity based feature representations. In K-means clustering, we have an input parameter k, which specifies the number of clusters it will output using the document features. This clustering method is a centroid based clustering method, where it tries to cluster these documents into clusters of equal variance. It tries to create these clusters by minimizing the within-cluster sum of squares measure, also known as inertia. ","metadata":{"id":"vh3zF8lKRP6n"}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=3, random_state=0)\nkm.fit_transform(similarity_matrix)\ncluster_labels = km.labels_\ncluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\npd.concat([corpus_df, cluster_labels], axis=1)","metadata":{"id":"l5ff9gMZRP6o","outputId":"865209d2-1755-4cda-82f3-669c7e4e224d","execution":{"iopub.status.busy":"2021-09-25T16:58:22.192272Z","iopub.execute_input":"2021-09-25T16:58:22.192606Z","iopub.status.idle":"2021-09-25T16:58:22.385014Z","shell.execute_reply.started":"2021-09-25T16:58:22.192575Z","shell.execute_reply":"2021-09-25T16:58:22.384230Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**We can see from the above output that our documents were correctly assigned to the right clusters!**","metadata":{"id":"S-NsBGloRP6q"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}